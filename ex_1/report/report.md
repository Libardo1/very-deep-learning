% Tweaking the AlexNet
% Amit Sahu; Ayushman Dash;
  John Gamboa; Vitor Rey
% November 8, 2016

# Introduction

The [AlexNet] is this and that...

# Tweaks

As a baseline, the original AlexNet (describe its exact size, mention it used
ReLU units) was trained.

We wanted to compare speed of convergence of the original AlexNet with that of
an AlexNet using $tanh$ units. In what follows, we call this model the
"$tanh$-AlexNet".

Additionally, we trained a much smaller model to investigate how better the
results of AlexNet would be compared to this cheapily trainable model. In what
follows, we call it "small-AlexNet". (describe its exact size).

Figure **??** shows the evolution of the loss of the original AlexNet.

[AlexNet]: https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf

# Discussion

The tahn was worse...

The small was not that terrible...


